{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VNxc-l_dqI8e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import jieba\n",
        "import scipy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "# Download \"dict.txt.big\" from https://github.com/fxsjy/jieba\n",
        "jieba.set_dictionary(\"../data/dict.txt.big\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "# Adjust the number of workers if you want\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from tfidf_utils import (\n",
        "    load_json,\n",
        "    jsonl_dir_to_df,\n",
        "    calculate_precision,\n",
        "    calculate_recall,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3_sPDPJ2qI8f"
      },
      "outputs": [],
      "source": [
        "# Get the stopwords\n",
        "# https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library\n",
        "from TCSP import read_stopwords_list\n",
        "\n",
        "stopwords = read_stopwords_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZWKt6BBSqI8g"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str, stopwords: list) -> str:\n",
        "\n",
        "    tokens = jieba.lcut(text)\n",
        "\n",
        "    return \" \".join([w for w in tokens if w not in stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JtqRJy35qI8h"
      },
      "outputs": [],
      "source": [
        "def get_pred_docs_sklearn(\n",
        "    claim: str,\n",
        "    tokenizing_method: callable,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    tf_idf_matrix: scipy.sparse.csr_matrix,\n",
        "    wiki_pages: pd.DataFrame,\n",
        "    topk: int,\n",
        ") -> set:\n",
        "    \n",
        "    tokens = tokenizing_method(claim)\n",
        "    claim_vector = vectorizer.transform([tokens])\n",
        "    similarity_scores = tf_idf_matrix.dot(claim_vector.T)\n",
        "\n",
        "    # `similarity_scores` shape: (num_wiki_pages x 1)\n",
        "    similarity_scores = similarity_scores.toarray()[:, 0]  # flatten the array\n",
        "\n",
        "    # Sort the similarity scores in descending order\n",
        "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
        "    topk_sorted_indices = sorted_indices[:topk]\n",
        "\n",
        "    # Get the wiki page names based on the topk sorted indices \n",
        "    results = wiki_pages.iloc[topk_sorted_indices][\"id\"]\n",
        "\n",
        "    exact_matchs = []\n",
        "    \n",
        "    # check if a result is exactly mentioned in the claim\n",
        "    for result in results:\n",
        "        if (\n",
        "            (result in claim)\n",
        "            or (result in claim.replace(\" \", \"\")) # E.g., MS DOS -> MSDOS\n",
        "            or (result.replace(\"·\", \"\") in claim) # E.g., 湯姆·克魯斯 -> 湯姆克魯斯\n",
        "            or (result.replace(\"-\", \"\") in claim) # E.g., X-SAMPA -> XSAMPA\n",
        "        ):\n",
        "            exact_matchs.append(result)\n",
        "        elif \"·\" in result:\n",
        "            splitted = result.split(\"·\") # E.g., 阿爾伯特·愛因斯坦 -> 愛因斯坦\n",
        "            for split in splitted:\n",
        "                if split in claim:\n",
        "                    exact_matchs.append(result)\n",
        "                    break\n",
        "\n",
        "    return set(exact_matchs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kZbng6TyqI8k"
      },
      "outputs": [],
      "source": [
        "wiki_cache = \"wiki\"\n",
        "target_column = \"text\"\n",
        "wiki_path = \"../data/wiki-pages\"\n",
        "\n",
        "wiki_cache_path = Path(f\"../data/{wiki_cache}.pkl\")\n",
        "if wiki_cache_path.exists():\n",
        "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
        "else:\n",
        "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
        "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
        "    # tokenize the text and keep the result in a new column `processed_text`\n",
        "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n",
        "        partial(tokenize, stopwords=stopwords)\n",
        "    )\n",
        "    # save the result to a pickle file\n",
        "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KjRzlQL0qI8l"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "min_wiki_length = 5\n",
        "topk = 5\n",
        "min_df = 0\n",
        "max_df = 0.8\n",
        "use_idf = True\n",
        "sublinear_tf = True\n",
        "norm = None\n",
        "\n",
        "# Build the TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=max_df,\n",
        "    min_df=min_df,\n",
        "    use_idf=use_idf,\n",
        "    sublinear_tf=sublinear_tf,\n",
        "    norm=norm,\n",
        "    stop_words=stopwords,\n",
        "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7oUXnEAIqI8m"
      },
      "outputs": [],
      "source": [
        "wiki_pages = wiki_pages[\n",
        "    wiki_pages['processed_text'].str.len() > min_wiki_length\n",
        "]\n",
        "corpus = wiki_pages[\"processed_text\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sM-mkBu5qI8m"
      },
      "outputs": [],
      "source": [
        "# Start to encode the corpus with TF-IDF\n",
        "X = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT_ovhYRYZNt",
        "outputId": "a8a8e176-627e-488f-d955-e127d8252ae6"
      },
      "outputs": [],
      "source": [
        "train = load_json(\"../data/public_train_new.jsonl\")\n",
        "train_df = pd.DataFrame(train)\n",
        "\n",
        "# Perform the prediction for document retrieval\n",
        "train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n",
        "    partial(\n",
        "        get_pred_docs_sklearn,\n",
        "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "        vectorizer=vectorizer,\n",
        "        tf_idf_matrix=X,\n",
        "        wiki_pages=wiki_pages,\n",
        "        topk=topk,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.6860980699008852\n",
            "Recall: 0.7865163917980814\n"
          ]
        }
      ],
      "source": [
        "precision = calculate_precision(train, train_df[\"predictions\"])\n",
        "recall = calculate_recall(train, train_df[\"predictions\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df[\"predictions\"].to_pickle('../data/train_tfidf_doc5.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 989/989 [07:59<00:00,  2.06it/s]\n"
          ]
        }
      ],
      "source": [
        "test = load_json(\"../data/public_test.jsonl\")\n",
        "test_df = pd.DataFrame(test)\n",
        "\n",
        "test_df[\"predictions\"] = test_df[\"claim\"].progress_apply(\n",
        "    partial(\n",
        "        get_pred_docs_sklearn,\n",
        "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "        vectorizer=vectorizer,\n",
        "        tf_idf_matrix=X,\n",
        "        wiki_pages=wiki_pages,\n",
        "        topk=topk,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df[\"predictions\"].to_pickle('../data/test_tfidf_doc5.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8049/8049 [1:06:38<00:00,  2.01it/s]\n"
          ]
        }
      ],
      "source": [
        "private = load_json(\"../data/private_test_data.jsonl\")\n",
        "private_df = pd.DataFrame(private)\n",
        "\n",
        "private_df[\"predictions\"] = private_df[\"claim\"].progress_apply(\n",
        "    partial(\n",
        "        get_pred_docs_sklearn,\n",
        "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "        vectorizer=vectorizer,\n",
        "        tf_idf_matrix=X,\n",
        "        wiki_pages=wiki_pages,\n",
        "        topk=topk,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "private_df[\"predictions\"].to_pickle('../data/private_tfidf_doc5.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.16 ('hw3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7cbde34494a11b1624bcc1eb71dba5ceb4f0ff8d7a6c820b0d6fa32591a5e209"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
